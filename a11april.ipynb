{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8690ffbc-1249-40ff-84ff-aba521b12378",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a8650-c919-451f-a9db-9b903eac3646",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning refers to the combination of multiple individual models to make more accurate predictions or classifications. It involves training multiple models on the same dataset using different algorithms or variations of the same algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210eefe-6cd0-46bd-8bd4-b4ae7c0d98fc",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4a78b-59e6-4118-b8cd-9caa44aa7257",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "They can improve the overall prediction accuracy and generalization of models.\n",
    "They help in reducing overfitting by combining the predictions of multiple models.\n",
    "They are capable of capturing different patterns and dependencies in the data that may be missed by individual models.\n",
    "Ensemble techniques can handle various types of data and models, making them versatile in different problem domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c3120-e21f-4c49-9c43-50bb23780a76",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a0e94-0376-43d5-b18b-42734685a555",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregating) is a popular ensemble technique that involves creating multiple subsets of the original dataset through resampling (sampling with replacement). Each subset is used to train a separate model, and the final prediction or classification is made by combining the predictions of all models, typically through majority voting or averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ea250-b0f0-437d-911c-667f4210dace",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f336c-d5a5-458a-835f-185c085d5797",
   "metadata": {},
   "source": [
    "Boosting is another ensemble technique where multiple weak models are combined to create a strong model. Unlike bagging, boosting involves training models sequentially, where each model focuses on correcting the mistakes made by the previous models. The final prediction is made by aggregating the predictions of all models, often weighted based on their individual performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766f165-0cad-4d4a-bd4a-a5b1966f2ef4",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fcb5dc-00dc-4a73-b1ab-c4d4ad1652be",
   "metadata": {},
   "source": [
    "There are several benefits of using ensemble techniques:\n",
    "\n",
    "Improved accuracy: Ensemble models can produce more accurate predictions than individual models, especially when the individual models have different strengths and weaknesses.\n",
    "Robustness: Ensemble models are typically more robust and less prone to overfitting because they combine the predictions of multiple models.\n",
    "Generalization: Ensemble techniques can capture a broader range of patterns and dependencies in the data, leading to better generalization and performance on unseen data.\n",
    "Versatility: Ensemble techniques can work well with different types of data and models, making them applicable to various machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad1ef7-de70-46c7-b0e2-f34be03c7c87",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df8309-e4c1-43e5-b70f-3d0df37250ac",
   "metadata": {},
   "source": [
    "While ensemble techniques often outperform individual models, they are not always guaranteed to be better. The performance of ensemble techniques depends on several factors, including the quality and diversity of the individual models, the nature of the problem, and the specific ensemble method employed. In some cases, a single well-tuned model may outperform an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829c1b2-5774-487d-a78a-8ac98eea4f08",
   "metadata": {},
   "source": [
    "# Quetion : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d531286-cfd7-4e51-a396-a70ae2411965",
   "metadata": {},
   "source": [
    "The confidence interval can be calculated using bootstrap by following these steps:\n",
    "\n",
    "Randomly sample the original dataset with replacement to create a new bootstrap sample of the same size as the original dataset.\n",
    "Calculate the mean (or any other statistic of interest) from the bootstrap sample.\n",
    "Repeat steps 1 and 2 a large number of times (e.g., 1,000 or 10,000) to create a distribution of bootstrap statistics.\n",
    "Compute the desired confidence interval from the distribution of bootstrap statistics, typically by taking the percentiles corresponding to the desired level of confidence (e.g., 2.5th and 97.5th percentiles for a 95% confidence interval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af5dd3-fb75-4348-b610-afc73080b6d2",
   "metadata": {},
   "source": [
    "# Quetion : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21cd0d-588b-43cf-8d00-cbe7e13a0e0c",
   "metadata": {},
   "source": [
    " Bootstrap is a resampling technique used for estimating the sampling distribution of a statistic or making inferences about a population parameter. The steps involved in bootstrap are as follows:\n",
    "\n",
    "Randomly sample the original dataset with replacement to create a new bootstrap sample of the same size as the original dataset.\n",
    "Perform the desired analysis or compute the statistic of interest on the bootstrap sample.\n",
    "Repeat steps 1 and 2 a large number of times (e.g., 1,000 or 10,000) to create a distribution of bootstrap statistics.\n",
    "Use the distribution of bootstrap statistics to estimate the sampling distribution of the statistic or make inferences such as confidence intervals or hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b2f25-83ef-447d-981c-925f6ef983f0",
   "metadata": {},
   "source": [
    "# Quetion : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47196c6-7a34-4f50-9e56-85cc78ab8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval:  [5.82 8.2 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "\n",
    "n_bootstrap = 10000\n",
    "\n",
    "\n",
    "bootstrap_samples = np.random.choice(sample_mean, size=(n_bootstrap, sample_size), replace=True)\n",
    "bootstrap_sample_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval: \", confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d503d-9b4b-4bf2-b480-fe8928b624d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
